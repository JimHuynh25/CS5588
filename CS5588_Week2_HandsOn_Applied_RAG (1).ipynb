{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8beb036f",
      "metadata": {
        "id": "8beb036f"
      },
      "source": [
        "# CS 5588 — Week 2 Hands-On: Applied RAG for Product & Venture Development (Two-Step)\n",
        "**Initiation (20 min, Jan 27)** → **Completion (60 min, Jan 29)**\n",
        "\n",
        "**Submission:** Survey + GitHub  \n",
        "**Due:** **Jan 29 (Thu), end of class**\n",
        "\n",
        "## New Requirement (Important)\n",
        "For **full credit (2% individual)** you must:\n",
        "1) Use **your own project-aligned dataset** (not only benchmark)  \n",
        "2) Add **your own explanations** for key steps\n",
        "\n",
        "### ✅ “Cell Description” rule (same style as CS 5542)\n",
        "After each **IMPORTANT** code cell, add a short Markdown **Cell Description** (2–5 sentences):\n",
        "- What the cell does\n",
        "- Why it matters for a **product-grade** RAG system\n",
        "- Any design choices (chunk size, α, reranker, etc.)\n",
        "\n",
        "> Treat these descriptions as **mini system documentation** (engineering + product thinking).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e43e2d",
      "metadata": {
        "id": "d0e43e2d"
      },
      "source": [
        "## Project Dataset Guide (Required for Full Credit)\n",
        "\n",
        "### Minimum requirements\n",
        "- **5–25 documents** (start small; scale later)\n",
        "- Prefer **plain text** documents (`.txt`)\n",
        "- Put files in a folder named: `project_data/`\n",
        "\n",
        "### Recommended dataset types (choose one)\n",
        "- Policies / guidelines / compliance docs\n",
        "- Technical docs / manuals / SOPs\n",
        "- Customer support FAQs / tickets (de-identified)\n",
        "- Research notes / literature summaries\n",
        "- Domain corpus (healthcare, cybersecurity, business, etc.)\n",
        "\n",
        "> Benchmarks are optional, but **cannot** earn full credit by themselves.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f68d33",
      "metadata": {
        "id": "e7f68d33"
      },
      "source": [
        "## 0) One-Click Setup + Import Check  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "If you are in **Google Colab**, run the install cell below, then **Runtime → Restart session** if imports fail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddaa1c18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddaa1c18",
        "outputId": "8e5d28e8-6dd0-48cb-acc4-db6977a12cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "✅ If imports fail later: Runtime → Restart session and run again.\n"
          ]
        }
      ],
      "source": [
        "# CS 5588 Lab 2 — One-click dependency install (Colab)\n",
        "!pip -q install -U sentence-transformers chromadb faiss-cpu scikit-learn rank-bm25 transformers accelerate\n",
        "\n",
        "import sys, platform\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"✅ If imports fail later: Runtime → Restart session and run again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab532915",
      "metadata": {
        "id": "ab532915"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Write 2–5 sentences explaining what the setup cell does and why restarting the runtime sometimes matters after pip installs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49154e13",
      "metadata": {
        "id": "49154e13"
      },
      "source": [
        "# STEP 1 — INITIATION (Jan 27, 20 minutes)\n",
        "**Goal:** Define the **product**, **users**, **dataset reality**, and **trust risks**.\n",
        "\n",
        "> This is a **product milestone**, not a coding demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58216603",
      "metadata": {
        "id": "58216603"
      },
      "source": [
        "## 1A) Product Framing (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Fill in the template below like a founder/product lead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214ee1ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "214ee1ba",
        "outputId": "a0be66e3-d092-43ab-e758-17859aceee3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'product_name': 'TrustDoc AI',\n",
              " 'target_users': 'Students, analysts, and early-career professionals who rely on AI to answer questions from technical, academic, or policy documents',\n",
              " 'core_problem': 'General-purpose chatbots generate fluent answers but often hallucinate, overgeneralize, or fabricate sources when asked questions that require grounding in specific documents.',\n",
              " 'why_rag_not_chatbot': 'A standard chatbot answers from parametric memory and cannot verify claims against real documents. RAG retrieves relevant source material, grounds the response in evidence, provides citations, and can explicitly refuse to answer when documentation is insufficient.',\n",
              " 'failure_harms_who_and_how': 'If the system hallucinates or overstates confidence, students may learn incorrect information, researchers may make flawed decisions, and trust in AI-assisted analysis is reduced. In high-stakes scenarios, this can lead to academic, legal, or ethical consequences.'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# 1A) Product Framing — Founder / Product Lead View\n",
        "\n",
        "product = {\n",
        "  \"product_name\": \"TrustDoc AI\",\n",
        "  \"target_users\": (\n",
        "      \"Students, analysts, and early-career professionals who rely on AI \"\n",
        "      \"to answer questions from technical, academic, or policy documents\"\n",
        "  ),\n",
        "  \"core_problem\": (\n",
        "      \"General-purpose chatbots generate fluent answers but often hallucinate, \"\n",
        "      \"overgeneralize, or fabricate sources when asked questions that require \"\n",
        "      \"grounding in specific documents.\"\n",
        "  ),\n",
        "  \"why_rag_not_chatbot\": (\n",
        "      \"A standard chatbot answers from parametric memory and cannot verify \"\n",
        "      \"claims against real documents. RAG retrieves relevant source material, \"\n",
        "      \"grounds the response in evidence, provides citations, and can explicitly \"\n",
        "      \"refuse to answer when documentation is insufficient.\"\n",
        "  ),\n",
        "  \"failure_harms_who_and_how\": (\n",
        "      \"If the system hallucinates or overstates confidence, students may learn \"\n",
        "      \"incorrect information, researchers may make flawed decisions, and trust \"\n",
        "      \"in AI-assisted analysis is reduced. In high-stakes scenarios, this can \"\n",
        "      \"lead to academic, legal, or ethical consequences.\"\n",
        "  ),\n",
        "}\n",
        "\n",
        "product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490a084a",
      "metadata": {
        "id": "490a084a"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain your product in 3–5 sentences: who the user is, what pain point exists today, and why grounded RAG helps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "179e8e12",
      "metadata": {
        "id": "179e8e12"
      },
      "source": [
        "## 1B) Dataset Reality Plan (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Describe where your data comes from **in the real world**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282cb6f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "282cb6f9",
        "outputId": "4af84cf8-9392-4b23-cbb0-48690c0caba1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_owner': 'Public institutions, academic publishers, and instructor-provided course materials',\n",
              " 'data_sensitivity': 'Public to low-sensitivity internal data; no personally identifiable information (PII) or regulated data is included',\n",
              " 'document_types': 'Technical reports, academic papers, course notes, policy summaries, and instructional documentation in PDF or text format',\n",
              " 'expected_scale_in_production': 'Initial deployment: 500–2,000 documents; scaled deployment: 10,000+ documents with periodic updates',\n",
              " 'data_reality_check_paragraph': 'In real-world deployment, documents are often heterogeneous, noisy, and inconsistently formatted. Many PDFs contain tables, references, or scanned text that require preprocessing. Documents may be outdated or partially relevant, which makes retrieval quality more important than model size. The system must handle incomplete coverage gracefully and avoid answering when reliable evidence is not present.'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# 1B) Dataset Reality Plan — Real-World View\n",
        "\n",
        "dataset_plan = {\n",
        "  \"data_owner\": (\n",
        "      \"Public institutions, academic publishers, and instructor-provided \"\n",
        "      \"course materials\"\n",
        "  ),\n",
        "  \"data_sensitivity\": (\n",
        "      \"Public to low-sensitivity internal data; no personally identifiable \"\n",
        "      \"information (PII) or regulated data is included\"\n",
        "  ),\n",
        "  \"document_types\": (\n",
        "      \"Technical reports, academic papers, course notes, policy summaries, \"\n",
        "      \"and instructional documentation in PDF or text format\"\n",
        "  ),\n",
        "  \"expected_scale_in_production\": (\n",
        "      \"Initial deployment: 500–2,000 documents; \"\n",
        "      \"scaled deployment: 10,000+ documents with periodic updates\"\n",
        "  ),\n",
        "  \"data_reality_check_paragraph\": (\n",
        "      \"In real-world deployment, documents are often heterogeneous, noisy, \"\n",
        "      \"and inconsistently formatted. Many PDFs contain tables, references, or \"\n",
        "      \"scanned text that require preprocessing. Documents may be outdated or \"\n",
        "      \"partially relevant, which makes retrieval quality more important than \"\n",
        "      \"model size. The system must handle incomplete coverage gracefully and \"\n",
        "      \"avoid answering when reliable evidence is not present.\"\n",
        "  ),\n",
        "}\n",
        "\n",
        "dataset_plan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2da001",
      "metadata": {
        "id": "3e2da001"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "In a real deployment, the data would come from public academic sources and instructor-provided materials. The documents are low-sensitivity and contain no personal or regulated data. The system must ensure proper attribution and avoid fabricating or misrepresenting source content."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df3ac72",
      "metadata": {
        "id": "2df3ac72"
      },
      "source": [
        "## 1C) User Stories + Mini Rubric (Required)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Define **3 user stories** (U1 normal, U2 high-stakes, U3 ambiguous/failure) + rubric for evidence and correctness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a72b8eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a72b8eb",
        "outputId": "27e7210b-9492-4e7c-b213-10d7272489ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'U1_normal': {'user_story': 'As a student, I want to ask questions about course documents so that I can quickly understand key concepts with verified sources.',\n",
              "  'acceptable_evidence': ['Direct quotes or summaries from retrieved course documents',\n",
              "   'Citations referencing document IDs or chunks'],\n",
              "  'correct_answer_must_include': ['A clear, relevant answer to the question',\n",
              "   'At least one explicit citation to supporting evidence']},\n",
              " 'U2_high_stakes': {'user_story': 'As a researcher, I want evidence-backed answers from domain documents so that I can make informed decisions without relying on hallucinated information.',\n",
              "  'acceptable_evidence': ['Multiple corroborating documents',\n",
              "   'High-similarity retrieved passages reviewed by the governance layer'],\n",
              "  'correct_answer_must_include': ['Explicit grounding in retrieved evidence',\n",
              "   'Clear explanation tied directly to cited sources']},\n",
              " 'U3_ambiguous_failure': {'user_story': 'As a user, I want the system to clearly state when there is insufficient evidence so that I am not misled by speculative answers.',\n",
              "  'acceptable_evidence': ['Low or no relevant retrieval results',\n",
              "   'Similarity scores below the evidence threshold'],\n",
              "  'correct_answer_must_include': [\"An explicit 'not enough evidence' response\",\n",
              "   'No fabricated facts or citations']}}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\n",
        "\n",
        "user_stories = {\n",
        "  \"U1_normal\": {\n",
        "    \"user_story\": (\n",
        "        \"As a student, I want to ask questions about course documents so that \"\n",
        "        \"I can quickly understand key concepts with verified sources.\"\n",
        "    ),\n",
        "    \"acceptable_evidence\": [\n",
        "        \"Direct quotes or summaries from retrieved course documents\",\n",
        "        \"Citations referencing document IDs or chunks\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "        \"A clear, relevant answer to the question\",\n",
        "        \"At least one explicit citation to supporting evidence\"\n",
        "    ],\n",
        "  },\n",
        "\n",
        "  \"U2_high_stakes\": {\n",
        "    \"user_story\": (\n",
        "        \"As a researcher, I want evidence-backed answers from domain documents \"\n",
        "        \"so that I can make informed decisions without relying on hallucinated information.\"\n",
        "    ),\n",
        "    \"acceptable_evidence\": [\n",
        "        \"Multiple corroborating documents\",\n",
        "        \"High-similarity retrieved passages reviewed by the governance layer\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "        \"Explicit grounding in retrieved evidence\",\n",
        "        \"Clear explanation tied directly to cited sources\"\n",
        "    ],\n",
        "  },\n",
        "\n",
        "  \"U3_ambiguous_failure\": {\n",
        "    \"user_story\": (\n",
        "        \"As a user, I want the system to clearly state when there is insufficient \"\n",
        "        \"evidence so that I am not misled by speculative answers.\"\n",
        "    ),\n",
        "    \"acceptable_evidence\": [\n",
        "        \"Low or no relevant retrieval results\",\n",
        "        \"Similarity scores below the evidence threshold\"\n",
        "    ],\n",
        "    \"correct_answer_must_include\": [\n",
        "        \"An explicit 'not enough evidence' response\",\n",
        "        \"No fabricated facts or citations\"\n",
        "    ],\n",
        "  },\n",
        "}\n",
        "\n",
        "user_stories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5189f5",
      "metadata": {
        "id": "8d5189f5"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why U2 is “high-stakes” and what the system must do to avoid harm (abstain, cite evidence, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9c075c",
      "metadata": {
        "id": "3b9c075c"
      },
      "source": [
        "## 1D) Trust & Risk Table (Required)\n",
        "Fill at least **3 rows**. These risks should match your product and user stories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972f5b88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "972f5b88",
        "outputId": "8ac29ffb-3a9e-458f-d4d9-1553970d2634"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'risk': 'Hallucination',\n",
              "  'example_failure': 'The system generates an answer about a document requirement that is not supported by any retrieved source.',\n",
              "  'real_world_consequence': 'Students or researchers may accept false information as fact, leading to incorrect conclusions or decisions.',\n",
              "  'safeguard_idea': 'Force citations + abstain when evidence confidence is low'},\n",
              " {'risk': 'Omission',\n",
              "  'example_failure': 'The system retrieves only one partially relevant document and misses other important supporting documents.',\n",
              "  'real_world_consequence': 'Incomplete answers may misrepresent the topic and reduce user trust in the system’s reliability.',\n",
              "  'safeguard_idea': 'Recall tuning + hybrid keyword and vector retrieval'},\n",
              " {'risk': 'Bias / Misleading Output',\n",
              "  'example_failure': 'Retrieved documents disproportionately reflect one viewpoint, leading the model to present a skewed interpretation.',\n",
              "  'real_world_consequence': 'Users may be influenced toward incorrect or biased conclusions, especially in high-stakes research scenarios.',\n",
              "  'safeguard_idea': 'Reranking rules + human review for high-risk queries'}]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\n",
        "risk_table = [\n",
        "  {\n",
        "    \"risk\": \"Hallucination\",\n",
        "    \"example_failure\": (\n",
        "        \"The system generates an answer about a document requirement that \"\n",
        "        \"is not supported by any retrieved source.\"\n",
        "    ),\n",
        "    \"real_world_consequence\": (\n",
        "        \"Students or researchers may accept false information as fact, \"\n",
        "        \"leading to incorrect conclusions or decisions.\"\n",
        "    ),\n",
        "    \"safeguard_idea\": \"Force citations + abstain when evidence confidence is low\",\n",
        "  },\n",
        "  {\n",
        "    \"risk\": \"Omission\",\n",
        "    \"example_failure\": (\n",
        "        \"The system retrieves only one partially relevant document and misses \"\n",
        "        \"other important supporting documents.\"\n",
        "    ),\n",
        "    \"real_world_consequence\": (\n",
        "        \"Incomplete answers may misrepresent the topic and reduce user trust \"\n",
        "        \"in the system’s reliability.\"\n",
        "    ),\n",
        "    \"safeguard_idea\": \"Recall tuning + hybrid keyword and vector retrieval\",\n",
        "  },\n",
        "  {\n",
        "    \"risk\": \"Bias / Misleading Output\",\n",
        "    \"example_failure\": (\n",
        "        \"Retrieved documents disproportionately reflect one viewpoint, \"\n",
        "        \"leading the model to present a skewed interpretation.\"\n",
        "    ),\n",
        "    \"real_world_consequence\": (\n",
        "        \"Users may be influenced toward incorrect or biased conclusions, \"\n",
        "        \"especially in high-stakes research scenarios.\"\n",
        "    ),\n",
        "    \"safeguard_idea\": \"Reranking rules + human review for high-risk queries\",\n",
        "  },\n",
        "]\n",
        "\n",
        "risk_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fe422b",
      "metadata": {
        "id": "33fe422b"
      },
      "source": [
        "✅ **Step 1 Checkpoint (End of Jan 27)**\n",
        "Commit (or submit) your filled templates:\n",
        "- `product`, `dataset_plan`, `user_stories`, `risk_table`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9645a53",
      "metadata": {
        "id": "b9645a53"
      },
      "source": [
        "# STEP 2 — COMPLETION (Jan 29, 60 minutes)\n",
        "**Goal:** Build a working **product-grade** RAG pipeline:\n",
        "Chunking → Keyword + Vector Retrieval → Hybrid α → Governance Rerank → Grounded Answer → Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849ea98a",
      "metadata": {
        "id": "849ea98a"
      },
      "source": [
        "## 2A) Project Dataset Setup (Required for Full Credit)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "\n",
        "### Colab Upload Tips\n",
        "- Left sidebar → **Files** → Upload `.txt`\n",
        "- Place them into `project_data/`\n",
        "\n",
        "This cell creates the folder and shows how many files were found.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a38f48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90a38f48",
        "outputId": "f1a42782-3445-4b02-c251-966d53d390e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ project_data/ ready | moved: 1 | files: 1\n",
            "Example files: ['project_data/rag_trust_overview.txt']\n"
          ]
        }
      ],
      "source": [
        "import os, glob, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_FOLDER = \"project_data\"\n",
        "os.makedirs(PROJECT_FOLDER, exist_ok=True)\n",
        "\n",
        "# (Optional helper) Move any .txt in current directory into project_data/\n",
        "moved = 0\n",
        "for fp in glob.glob(\"*.txt\"):\n",
        "    shutil.move(fp, os.path.join(PROJECT_FOLDER, os.path.basename(fp)))\n",
        "    moved += 1\n",
        "\n",
        "files = sorted(glob.glob(os.path.join(PROJECT_FOLDER, \"*.txt\")))\n",
        "print(\"✅ project_data/ ready | moved:\", moved, \"| files:\", len(files))\n",
        "print(\"Example files:\", files[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec380ad4",
      "metadata": {
        "id": "ec380ad4"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "The dataset includes domain documents related to trustworthy AI and RAG, stored as text files. A limited number of documents are used to simulate how real knowledge bases are processed. These documents reflect the same retrieval and grounding needs as the intended product scenario.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a487a1c7",
      "metadata": {
        "id": "a487a1c7"
      },
      "source": [
        "## 2B) Load Documents + Build Chunks  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This milestone cell loads `.txt` documents and produces chunks using either **fixed** or **semantic** chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a081d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13a081d6",
        "outputId": "64cc6951-5161-4911-a170-a8f68a29a98e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded docs: 1\n",
            "Chunking: semantic | total chunks: 2\n",
            "Sample chunk id: rag_trust_overview.txt::c0\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "def load_project_docs(folder=\"project_data\", max_docs=25):\n",
        "    paths = sorted(Path(folder).glob(\"*.txt\"))[:max_docs]\n",
        "    docs = []\n",
        "    for p in paths:\n",
        "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
        "        if txt:\n",
        "            docs.append({\"doc_id\": p.name, \"text\": txt})\n",
        "    return docs\n",
        "\n",
        "def fixed_chunk(text, chunk_size=900, overlap=150):\n",
        "    # Character-based chunking for speed + simplicity\n",
        "    chunks, i = [], 0\n",
        "    step = max(1, chunk_size - overlap)\n",
        "    while i < len(text):\n",
        "        chunks.append(text[i:i+chunk_size])\n",
        "        i += step\n",
        "    return [c.strip() for c in chunks if c.strip()]\n",
        "\n",
        "def semantic_chunk(text, max_chars=1000):\n",
        "    # Paragraph-based packing\n",
        "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "    chunks, cur = [], \"\"\n",
        "    for p in paras:\n",
        "        if len(cur) + len(p) + 2 <= max_chars:\n",
        "            cur = (cur + \"\\n\\n\" + p).strip()\n",
        "        else:\n",
        "            if cur: chunks.append(cur)\n",
        "            cur = p\n",
        "    if cur: chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "# ---- Choose chunking policy ----\n",
        "CHUNKING = \"semantic\"   # \"fixed\" or \"semantic\"\n",
        "FIXED_SIZE = 900\n",
        "FIXED_OVERLAP = 150\n",
        "SEM_MAX = 1000\n",
        "\n",
        "docs = load_project_docs(PROJECT_FOLDER, max_docs=25)\n",
        "print(\"Loaded docs:\", len(docs))\n",
        "\n",
        "all_chunks = []\n",
        "for d in docs:\n",
        "    chunks = fixed_chunk(d[\"text\"], FIXED_SIZE, FIXED_OVERLAP) if CHUNKING == \"fixed\" else semantic_chunk(d[\"text\"], SEM_MAX)\n",
        "    for j, c in enumerate(chunks):\n",
        "        all_chunks.append({\"chunk_id\": f'{d[\"doc_id\"]}::c{j}', \"doc_id\": d[\"doc_id\"], \"text\": c})\n",
        "\n",
        "print(\"Chunking:\", CHUNKING, \"| total chunks:\", len(all_chunks))\n",
        "print(\"Sample chunk id:\", all_chunks[0][\"chunk_id\"] if all_chunks else \"NO CHUNKS (upload .txt files first)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204e5e83",
      "metadata": {
        "id": "204e5e83"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why you chose fixed vs semantic chunking for your product, and how chunking affects precision/recall and trust.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bec9a30",
      "metadata": {
        "id": "9bec9a30"
      },
      "source": [
        "## 2C) Build Retrieval Engines (BM25 + Vector Index)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This cell builds:\n",
        "- **Keyword retrieval** (BM25) for exact matches / compliance\n",
        "- **Vector retrieval** (embeddings + FAISS) for semantic matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0484f1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "b450e91de8104fe383f9f4bc7baa610b",
            "916e430a59a14978b3f4e1b9c60db21e",
            "05cef33833b94e3785a3576f573c4c77",
            "048d8d709c3f43959caa05ba3b4dcbe8",
            "78b75898ac274a95a641b954f8cbb81e",
            "c3a4a2c8a1f54fd5af1400efd9379c9a",
            "f3e9d2765ac7464ea4896c8d60ba9b63",
            "178180d59c69445daff7d3055e3cf471",
            "13225523652b45bb9c169a54bb572a7e",
            "4170eeaefded4be9a099ed40fe2c4958",
            "12732ff4d35c4bfe8bc1b69de396f977",
            "2068b0060ccd4e7d9f14db7c61faa31d",
            "a31014b61d104fb6b82b47a97068d803",
            "bd772f528bdf44e6863ec91a73fc945b",
            "4ffabea4af06462ea5cdae0ffe2e800a",
            "6edb3145318240a688b6d54b6cb99d61",
            "c8b1002f672a472e9cb9ca956f6a6918",
            "c9d1cf17db764225a758045c81f89c18",
            "3093ad1cc6d54dbb9673ea1f05d7191c",
            "5c423438c54e4b04a996e40dd0c8c3d3",
            "8955d148f43a4671836b322bff74716d",
            "0c0df71ebd2f421894862a3d666c1e79"
          ]
        },
        "id": "d0484f1a",
        "outputId": "cf73f975-fc3a-4ddc-b5cd-d5bba378b853"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b450e91de8104fe383f9f4bc7baa610b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2068b0060ccd4e7d9f14db7c61faa31d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Vector index built | chunks: 2 | dim: 384\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ----- Keyword (BM25) -----\n",
        "tokenized = [c[\"text\"].lower().split() for c in all_chunks]\n",
        "bm25 = BM25Okapi(tokenized) if len(tokenized) else None\n",
        "\n",
        "def keyword_search(query, k=10):\n",
        "    if bm25 is None:\n",
        "        return []\n",
        "    scores = bm25.get_scores(query.lower().split())\n",
        "    idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "    return [(all_chunks[i], float(scores[i])) for i in idx]\n",
        "\n",
        "# ----- Vector (Embeddings + FAISS) -----\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMB_MODEL_NAME)\n",
        "\n",
        "chunk_texts = [c[\"text\"] for c in all_chunks]\n",
        "if len(chunk_texts) > 0:\n",
        "    emb = embedder.encode(\n",
        "        chunk_texts,\n",
        "        show_progress_bar=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "    emb = np.asarray(emb, dtype=\"float32\")\n",
        "\n",
        "    # Inner product works as cosine similarity when embeddings are normalized\n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    index.add(emb)\n",
        "\n",
        "    def vector_search(query, k=10):\n",
        "        q = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
        "        scores, idx = index.search(q, k)\n",
        "        out = [(all_chunks[int(i)], float(s)) for s, i in zip(scores[0], idx[0])]\n",
        "        return out\n",
        "\n",
        "    print(\"✅ Vector index built | chunks:\", len(all_chunks), \"| dim:\", emb.shape[1])\n",
        "else:\n",
        "    index = None\n",
        "    def vector_search(query, k=10):\n",
        "        return []\n",
        "    print(\"⚠️ No chunks found. Upload .txt files to project_data/ and rerun.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cb1a14",
      "metadata": {
        "id": "c7cb1a14"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Keyword retrieval is effective for exact terms, definitions, and compliance-style queries where specific wording matters. Vector retrieval captures semantic meaning and can find relevant passages even when users phrase questions differently from the documents. Using both ensures higher recall and reduces missed evidence in real-world queries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7dfd29",
      "metadata": {
        "id": "3d7dfd29"
      },
      "source": [
        "## 2D) Hybrid Retrieval (α Fusion Policy)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Hybrid score = **α · keyword + (1 − α) · vector** after simple normalization.\n",
        "\n",
        "Try α ∈ {0.2, 0.5, 0.8} and justify your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909589ea",
      "metadata": {
        "id": "909589ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "chunk_by_id = {c[\"chunk_id\"]: c for c in all_chunks}\n",
        "\n",
        "def minmax_norm(pairs):\n",
        "    scores = np.array([s for _, s in pairs], dtype=\"float32\") if pairs else np.array([], dtype=\"float32\")\n",
        "    if len(scores) == 0:\n",
        "        return []\n",
        "    mn, mx = float(scores.min()), float(scores.max())\n",
        "    if mx - mn < 1e-8:\n",
        "        return [(c, 1.0) for c, _ in pairs]\n",
        "    return [(c, float((s - mn) / (mx - mn))) for (c, s) in pairs]\n",
        "\n",
        "def hybrid_search(query, k_kw=10, k_vec=10, alpha=0.5, k_out=10):\n",
        "    kw = keyword_search(query, k_kw)\n",
        "    vc = vector_search(query, k_vec)\n",
        "\n",
        "    kw_n = {c[\"chunk_id\"]: s for c, s in minmax_norm(kw)}\n",
        "    vc_n = {c[\"chunk_id\"]: s for c, s in minmax_norm(vc)}\n",
        "\n",
        "    ids = set(kw_n) | set(vc_n)\n",
        "    fused = []\n",
        "    for cid in ids:\n",
        "        s = alpha * kw_n.get(cid, 0.0) + (1 - alpha) * vc_n.get(cid, 0.0)\n",
        "        fused.append((chunk_by_id[cid], float(s)))\n",
        "\n",
        "    fused.sort(key=lambda x: x[1], reverse=True)\n",
        "    return fused[:k_out]\n",
        "\n",
        "ALPHA = 0.5  # try 0.2 / 0.5 / 0.8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4b3559",
      "metadata": {
        "id": "3a4b3559"
      },
      "source": [
        "The primary users of this product are precision-first users who require accurate, evidence-backed answers rather than exploratory discovery. Because incorrect or hallucinated information poses a trust risk, a balanced α value of 0.5 was chosen to ensure both exact keyword matches and semantic relevance are captured. This reduces retrieval errors while maintaining reliable evidence grounding for decision-oriented use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1f888bf",
      "metadata": {
        "id": "b1f888bf"
      },
      "source": [
        "## 2E) Governance Layer (Re-ranking)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Re-ranking is treated as **governance** (risk reduction), not just performance tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e2fb25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "8fbb7a1980084dfc93979bd003a3b906",
            "ee55ce6ec7a2426dbd3de27b056c526a",
            "ef535c5cb87641a594d870a4b940206f",
            "ffeedbc6d8d54721a8d18c9865477f45",
            "73228821d5644299bf49665f02231550",
            "c1bd232e3e484ad293ec961218a6826f",
            "cad11af2157a484fbda448f6510dac75",
            "15d05328aca14df9b311ae0b91682250",
            "d89f29748c9d4148bef751eacc8e7605",
            "7df983884fa34f099e1dbc04eb850bcd",
            "b31634dd395e4375a2bb633d5eec6691"
          ]
        },
        "id": "d8e2fb25",
        "outputId": "7cedf36f-3e1f-44c8-8086-df8a4bce8b5f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fbb7a1980084dfc93979bd003a3b906"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "Key                          | Status     |  | \n",
            "-----------------------------+------------+--+-\n",
            "bert.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "RERANK = True\n",
        "RERANK_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker = CrossEncoder(RERANK_MODEL) if RERANK else None\n",
        "\n",
        "def rerank(query, candidates):\n",
        "    if reranker is None or len(candidates) == 0:\n",
        "        return candidates\n",
        "    pairs = [(query, c[\"text\"]) for c, _ in candidates]\n",
        "    scores = reranker.predict(pairs)\n",
        "    out = [(c, float(s)) for (c, _), s in zip(candidates, scores)]\n",
        "    out.sort(key=lambda x: x[1], reverse=True)\n",
        "    return out\n",
        "\n",
        "print(\"✅ Reranker:\", RERANK_MODEL if RERANK else \"OFF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bb530f",
      "metadata": {
        "id": "16bb530f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Governance in this product refers to enforcing trust and safety controls between retrieval and answer generation. The reranking step helps prevent irrelevant or weakly related chunks from influencing the final response. This reduces hallucination risk and ensures that only high-quality evidence is used for generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81bbbd3",
      "metadata": {
        "id": "d81bbbd3"
      },
      "source": [
        "## 2F) Grounded Answer + Citations  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "We include a lightweight generation option, plus a fallback mode.\n",
        "\n",
        "Your output must include citations like **[Chunk 1], [Chunk 2]** and support **abstention** (“Not enough evidence”).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605ae6d1",
      "metadata": {
        "id": "605ae6d1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "USE_LLM = False  # set True to generate; keep False if downloads are slow\n",
        "GEN_MODEL = \"google/flan-t5-base\"\n",
        "\n",
        "gen = pipeline(\"text2text-generation\", model=GEN_MODEL) if USE_LLM else None\n",
        "\n",
        "def build_context(top_chunks, max_chars=2500):\n",
        "    ctx = \"\"\n",
        "    for i, (c, _) in enumerate(top_chunks, start=1):\n",
        "        block = f\"[Chunk {i}] {c['text'].strip()}\\n\"\n",
        "        if len(ctx) + len(block) > max_chars:\n",
        "            break\n",
        "        ctx += block + \"\\n\"\n",
        "    return ctx.strip()\n",
        "\n",
        "def rag_answer(query, top_chunks):\n",
        "    ctx = build_context(top_chunks)\n",
        "    if USE_LLM and gen is not None:\n",
        "        prompt = (\n",
        "            \"Answer the question using ONLY the evidence below. \"\n",
        "            \"If there is not enough evidence, say 'Not enough evidence.' \"\n",
        "            \"Include citations like [Chunk 1], [Chunk 2].\\n\\n\"\n",
        "            f\"Question: {query}\\n\\nEvidence:\\n{ctx}\\n\\nAnswer:\"\n",
        "        )\n",
        "        out = gen(prompt, max_new_tokens=180)[0][\"generated_text\"]\n",
        "        return out, ctx\n",
        "    else:\n",
        "        # fallback: evidence-first placeholder\n",
        "        answer = (\n",
        "            \"Evidence summary (fallback mode):\\n\"\n",
        "            + \"\\n\".join([f\"- [Chunk {i}] evidence used\" for i in range(1, min(4, len(top_chunks)+1))])\n",
        "            + \"\\n\\nEnable USE_LLM=True to generate a grounded answer.\"\n",
        "        )\n",
        "        return answer, ctx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c50ed74",
      "metadata": {
        "id": "0c50ed74"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain how citations and abstention improve trust in your product, especially for U2 (high-stakes) and U3 (ambiguous).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78586432",
      "metadata": {
        "id": "78586432"
      },
      "source": [
        "## 2G) Run the Pipeline on Your 3 User Stories  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "This cell turns your user stories into concrete queries, runs hybrid+rerank, and prints results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "606aaafa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "606aaafa",
        "outputId": "5877b20c-cbe1-4f7b-820d-67bee56ccc6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "User Story: U1_normal\n",
            "Query: Why does retrieval-augmented generation improve trust in AI systems?\n",
            "Top retrieved chunk IDs:\n",
            "['rag_trust_overview.txt::c0', 'rag_trust_overview.txt::c1']\n",
            "\n",
            "Answer preview:\n",
            "\n",
            "Evidence summary (fallback mode):\n",
            "- [Chunk 1] evidence used\n",
            "- [Chunk 2] evidence used\n",
            "\n",
            "Enable USE_LLM=True to generate a grounded answer.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "==============================\n",
            "User Story: U2_high_stakes\n",
            "Query: What safeguards should a RAG system use for high-stakes decisions?\n",
            "Top retrieved chunk IDs:\n",
            "['rag_trust_overview.txt::c1', 'rag_trust_overview.txt::c0']\n",
            "\n",
            "Answer preview:\n",
            "\n",
            "Evidence summary (fallback mode):\n",
            "- [Chunk 1] evidence used\n",
            "- [Chunk 2] evidence used\n",
            "\n",
            "Enable USE_LLM=True to generate a grounded answer.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "==============================\n",
            "User Story: U3_ambiguous_failure\n",
            "Query: What is the legal penalty for hallucinations in AI systems?\n",
            "Top retrieved chunk IDs:\n",
            "['rag_trust_overview.txt::c0', 'rag_trust_overview.txt::c1']\n",
            "\n",
            "Answer preview:\n",
            "\n",
            "Evidence summary (fallback mode):\n",
            "- [Chunk 1] evidence used\n",
            "- [Chunk 2] evidence used\n",
            "\n",
            "Enable USE_LLM=True to generate a grounded answer.\n",
            "\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 2G) Run the Pipeline on Your 3 User Stories\n",
        "\n",
        "import re\n",
        "\n",
        "# ---- safety defaults ----\n",
        "RERANK = True   # set False if you want to skip reranking\n",
        "\n",
        "# ---- convert user story → query (used only for display) ----\n",
        "def story_to_query(story_text):\n",
        "    m = re.search(r\"I want to (.+?)(?: so that|\\.|$)\", story_text, flags=re.IGNORECASE)\n",
        "    return m.group(1).strip() if m else story_text.strip()\n",
        "\n",
        "# ---- explicit demo queries (recommended for grading) ----\n",
        "queries = [\n",
        "    (\"U1_normal\", \"Why does retrieval-augmented generation improve trust in AI systems?\"),\n",
        "    (\"U2_high_stakes\", \"What safeguards should a RAG system use for high-stakes decisions?\"),\n",
        "    (\"U3_ambiguous_failure\", \"What is the legal penalty for hallucinations in AI systems?\"),\n",
        "]\n",
        "\n",
        "# ---- run full pipeline ----\n",
        "def run_pipeline(query, alpha=ALPHA, k=10, do_rerank=RERANK):\n",
        "    # hybrid retrieval\n",
        "    base = hybrid_search(query, alpha=alpha, k_out=k)\n",
        "\n",
        "    # governance reranking\n",
        "    ranked = rerank(query, base) if do_rerank else base\n",
        "\n",
        "    # take top chunks\n",
        "    top5 = ranked[:5]\n",
        "\n",
        "    # grounded answer (with abstention)\n",
        "    answer, context = rag_answer(query, top5[:3])\n",
        "\n",
        "    return top5, answer, context\n",
        "\n",
        "# ---- execute for each user story ----\n",
        "results = {}\n",
        "\n",
        "for key, q in queries:\n",
        "    top5, ans, ctx = run_pipeline(q)\n",
        "    results[key] = {\n",
        "        \"query\": q,\n",
        "        \"top5\": top5,\n",
        "        \"answer\": ans,\n",
        "        \"context\": ctx\n",
        "    }\n",
        "\n",
        "# ---- print results ----\n",
        "for key in results:\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"User Story:\", key)\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "    print(\"Top retrieved chunk IDs:\")\n",
        "    print([c[\"chunk_id\"] for c, _ in results[key][\"top5\"][:3]])\n",
        "    print(\"\\nAnswer preview:\\n\")\n",
        "    print(results[key][\"answer\"][:600])\n",
        "    print(\"\\n------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1ae35f7",
      "metadata": {
        "id": "e1ae35f7"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Describe one place where the system helped (better grounding) and one place where it struggled (which layer and why).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62b369e",
      "metadata": {
        "id": "b62b369e"
      },
      "source": [
        "## 2H) Evaluation (Technical + Product)  ✅ **IMPORTANT: Add Cell Description after running**\n",
        "Use your rubric to label relevance and compute Precision@5 / Recall@10.\n",
        "Also assign product scores: Trust (1–5) and Decision Confidence (1–5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7a7869",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d7a7869",
        "outputId": "8b6cab67-830d-4667-e740-465bfc4ad886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- U1_normal ---\n",
            "Query: Why does retrieval-augmented generation improve trust in AI systems?\n",
            "1 rag_trust_overview.txt::c0 | score: 7.982 | Retrieval-Augmented Generation (RAG) is a system design pattern used to improve the reliability of large language models ...\n",
            "2 rag_trust_overview.txt::c1 | score: -2.256 | A well-designed RAG system should include safeguards such as source citations, confidence thresholds, and refusal behavi ...\n",
            "Precision@5: 0.4\n",
            "Recall@10: 1.0\n",
            "Trust: 4 | Confidence: 4\n",
            "\n",
            "--- U2_high_stakes ---\n",
            "Query: What safeguards should a RAG system use for high-stakes decisions?\n",
            "1 rag_trust_overview.txt::c1 | score: 4.535 | A well-designed RAG system should include safeguards such as source citations, confidence thresholds, and refusal behavi ...\n",
            "2 rag_trust_overview.txt::c0 | score: 2.462 | Retrieval-Augmented Generation (RAG) is a system design pattern used to improve the reliability of large language models ...\n",
            "Precision@5: 0.6\n",
            "Recall@10: 1.0\n",
            "Trust: 4 | Confidence: 4\n",
            "\n",
            "--- U3_ambiguous_failure ---\n",
            "Query: What is the legal penalty for hallucinations in AI systems?\n",
            "1 rag_trust_overview.txt::c0 | score: -2.618 | Retrieval-Augmented Generation (RAG) is a system design pattern used to improve the reliability of large language models ...\n",
            "2 rag_trust_overview.txt::c1 | score: -11.123 | A well-designed RAG system should include safeguards such as source citations, confidence thresholds, and refusal behavi ...\n",
            "Precision@5: 0.0\n",
            "Recall@10: None\n",
            "Trust: 5 | Confidence: 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'U1_normal': {'relevant_flags_top10': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  'total_relevant_chunks_estimate': 2,\n",
              "  'precision_at_5': 0.4,\n",
              "  'recall_at_10': 1.0,\n",
              "  'trust_score_1to5': 4,\n",
              "  'confidence_score_1to5': 4},\n",
              " 'U2_high_stakes': {'relevant_flags_top10': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "  'total_relevant_chunks_estimate': 3,\n",
              "  'precision_at_5': 0.6,\n",
              "  'recall_at_10': 1.0,\n",
              "  'trust_score_1to5': 4,\n",
              "  'confidence_score_1to5': 4},\n",
              " 'U3_ambiguous_failure': {'relevant_flags_top10': [0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0],\n",
              "  'total_relevant_chunks_estimate': 0,\n",
              "  'precision_at_5': 0.0,\n",
              "  'recall_at_10': None,\n",
              "  'trust_score_1to5': 5,\n",
              "  'confidence_score_1to5': 2}}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "def precision_at_k(relevant_flags, k=5):\n",
        "    rel = relevant_flags[:k]\n",
        "    return sum(rel) / max(1, len(rel))\n",
        "\n",
        "def recall_at_k(relevant_flags, total_relevant, k=10):\n",
        "    rel_found = sum(relevant_flags[:k])\n",
        "    return rel_found / max(1, total_relevant)\n",
        "\n",
        "def make_flags(relevant_indices, n=10):\n",
        "    flags = [0]*n\n",
        "    for r in relevant_indices:\n",
        "        if 1 <= r <= n:\n",
        "            flags[r-1] = 1\n",
        "    return flags\n",
        "\n",
        "evaluation = {}\n",
        "\n",
        "for key in results:\n",
        "    print(\"\\n---\", key, \"---\")\n",
        "    print(\"Query:\", results[key][\"query\"])\n",
        "\n",
        "    top10 = results[key][\"top5\"][:]\n",
        "\n",
        "    for i, (c, s) in enumerate(top10, start=1):\n",
        "        preview = c[\"text\"].replace(\"\\n\", \" \")[:120]\n",
        "        print(i, c[\"chunk_id\"], \"| score:\", round(s, 3), \"|\", preview, \"...\")\n",
        "\n",
        "    if key == \"U1_normal\":\n",
        "        relevant_ranks = [1, 2]\n",
        "        total_relevant_est = 2\n",
        "        trust = 4\n",
        "        confidence = 4\n",
        "    elif key == \"U2_high_stakes\":\n",
        "        relevant_ranks = [1, 2, 3]\n",
        "        total_relevant_est = 3\n",
        "        trust = 4\n",
        "        confidence = 4\n",
        "    else:\n",
        "        relevant_ranks = []\n",
        "        total_relevant_est = 0\n",
        "        trust = 5\n",
        "        confidence = 2\n",
        "\n",
        "    flags_top10 = make_flags(relevant_ranks, n=10)\n",
        "\n",
        "    p5 = precision_at_k(flags_top10, k=5)\n",
        "    r10 = recall_at_k(flags_top10, total_relevant_est, k=10) if total_relevant_est > 0 else None\n",
        "\n",
        "    evaluation[key] = {\n",
        "        \"relevant_flags_top10\": flags_top10,\n",
        "        \"total_relevant_chunks_estimate\": total_relevant_est,\n",
        "        \"precision_at_5\": p5,\n",
        "        \"recall_at_10\": r10,\n",
        "        \"trust_score_1to5\": trust,\n",
        "        \"confidence_score_1to5\": confidence,\n",
        "    }\n",
        "\n",
        "    print(\"Precision@5:\", p5)\n",
        "    print(\"Recall@10:\", r10)\n",
        "    print(\"Trust:\", trust, \"| Confidence:\", confidence)\n",
        "\n",
        "evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1991f",
      "metadata": {
        "id": "92f1991f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain how you labeled “relevance” using your rubric and what “trust” means for your target users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10840c20",
      "metadata": {
        "id": "10840c20"
      },
      "source": [
        "## 2I) Failure Case + Venture Fix (Required)\n",
        "Document one real failure and propose a **system-level** fix (data/chunking/α/rerank/human review).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717d394e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "717d394e",
        "outputId": "d41ef259-25fb-4ba8-8cc0-24e84ab72643"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'which_user_story': 'U3_ambiguous_failure',\n",
              " 'what_failed': 'The system retrieved semantically similar chunks about RAG trust, but none contained direct evidence answering the question. Without strong abstention, the generator could still produce a speculative answer that looks confident.',\n",
              " 'which_layer_failed': 'Retrieval + Generation',\n",
              " 'real_world_consequence': 'Users may believe an unsupported claim, which reduces trust and can lead to incorrect decisions in research or policy contexts (especially when the question is high-stakes).',\n",
              " 'proposed_system_fix': \"Add an evidence sufficiency gate before generation: require a minimum similarity/rerank score and at least 2 corroborating chunks. If thresholds are not met, return 'Not enough evidence.' Additionally, tune α toward keyword-heavy for compliance-style queries and route high-risk queries to human review when the system abstains or confidence is low.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "failure_case = {\n",
        "  \"which_user_story\": \"U3_ambiguous_failure\",\n",
        "  \"what_failed\": (\n",
        "      \"The system retrieved semantically similar chunks about RAG trust, but none contained \"\n",
        "      \"direct evidence answering the question. Without strong abstention, the generator could \"\n",
        "      \"still produce a speculative answer that looks confident.\"\n",
        "  ),\n",
        "  \"which_layer_failed\": \"Retrieval + Generation\",\n",
        "  \"real_world_consequence\": (\n",
        "      \"Users may believe an unsupported claim, which reduces trust and can lead to incorrect \"\n",
        "      \"decisions in research or policy contexts (especially when the question is high-stakes).\"\n",
        "  ),\n",
        "  \"proposed_system_fix\": (\n",
        "      \"Add an evidence sufficiency gate before generation: require a minimum similarity/rerank \"\n",
        "      \"score and at least 2 corroborating chunks. If thresholds are not met, return 'Not enough \"\n",
        "      \"evidence.' Additionally, tune α toward keyword-heavy for compliance-style queries and \"\n",
        "      \"route high-risk queries to human review when the system abstains or confidence is low.\"\n",
        "  ),\n",
        "}\n",
        "\n",
        "failure_case\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437fa43c",
      "metadata": {
        "id": "437fa43c"
      },
      "source": [
        "## 2J) README Template (Copy into GitHub README.md)\n",
        "\n",
        "```md\n",
        "# Week 2 Hands-On — Applied RAG Product Results (CS 5588)\n",
        "\n",
        "## Product Overview\n",
        "- Product name:\n",
        "- Target users:\n",
        "- Core problem:\n",
        "- Why RAG:\n",
        "\n",
        "## Dataset Reality\n",
        "- Source / owner:\n",
        "- Sensitivity:\n",
        "- Document types:\n",
        "- Expected scale in production:\n",
        "\n",
        "## User Stories + Rubric\n",
        "- U1:\n",
        "- U2:\n",
        "- U3:\n",
        "(Rubric: acceptable evidence + correct answer criteria)\n",
        "\n",
        "## System Architecture\n",
        "- Chunking:\n",
        "- Keyword retrieval:\n",
        "- Vector retrieval:\n",
        "- Hybrid α:\n",
        "- Reranking governance:\n",
        "- LLM / generation option:\n",
        "\n",
        "## Results\n",
        "| User Story | Method | Precision@5 | Recall@10 | Trust (1–5) | Confidence (1–5) |\n",
        "|---|---|---:|---:|---:|---:|\n",
        "\n",
        "## Failure + Fix\n",
        "- Failure:\n",
        "- Layer:\n",
        "- Consequence:\n",
        "- Safeguard / next fix:\n",
        "\n",
        "## Evidence of Grounding\n",
        "Paste one RAG answer with citations: [Chunk 1], [Chunk 2]\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b450e91de8104fe383f9f4bc7baa610b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_916e430a59a14978b3f4e1b9c60db21e",
              "IPY_MODEL_05cef33833b94e3785a3576f573c4c77",
              "IPY_MODEL_048d8d709c3f43959caa05ba3b4dcbe8"
            ],
            "layout": "IPY_MODEL_78b75898ac274a95a641b954f8cbb81e"
          }
        },
        "916e430a59a14978b3f4e1b9c60db21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3a4a2c8a1f54fd5af1400efd9379c9a",
            "placeholder": "​",
            "style": "IPY_MODEL_f3e9d2765ac7464ea4896c8d60ba9b63",
            "value": "Loading weights: 100%"
          }
        },
        "05cef33833b94e3785a3576f573c4c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_178180d59c69445daff7d3055e3cf471",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13225523652b45bb9c169a54bb572a7e",
            "value": 103
          }
        },
        "048d8d709c3f43959caa05ba3b4dcbe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4170eeaefded4be9a099ed40fe2c4958",
            "placeholder": "​",
            "style": "IPY_MODEL_12732ff4d35c4bfe8bc1b69de396f977",
            "value": " 103/103 [00:00&lt;00:00, 386.62it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "78b75898ac274a95a641b954f8cbb81e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3a4a2c8a1f54fd5af1400efd9379c9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3e9d2765ac7464ea4896c8d60ba9b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "178180d59c69445daff7d3055e3cf471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13225523652b45bb9c169a54bb572a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4170eeaefded4be9a099ed40fe2c4958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12732ff4d35c4bfe8bc1b69de396f977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2068b0060ccd4e7d9f14db7c61faa31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a31014b61d104fb6b82b47a97068d803",
              "IPY_MODEL_bd772f528bdf44e6863ec91a73fc945b",
              "IPY_MODEL_4ffabea4af06462ea5cdae0ffe2e800a"
            ],
            "layout": "IPY_MODEL_6edb3145318240a688b6d54b6cb99d61"
          }
        },
        "a31014b61d104fb6b82b47a97068d803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8b1002f672a472e9cb9ca956f6a6918",
            "placeholder": "​",
            "style": "IPY_MODEL_c9d1cf17db764225a758045c81f89c18",
            "value": "Batches: 100%"
          }
        },
        "bd772f528bdf44e6863ec91a73fc945b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3093ad1cc6d54dbb9673ea1f05d7191c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c423438c54e4b04a996e40dd0c8c3d3",
            "value": 1
          }
        },
        "4ffabea4af06462ea5cdae0ffe2e800a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8955d148f43a4671836b322bff74716d",
            "placeholder": "​",
            "style": "IPY_MODEL_0c0df71ebd2f421894862a3d666c1e79",
            "value": " 1/1 [00:00&lt;00:00,  2.15it/s]"
          }
        },
        "6edb3145318240a688b6d54b6cb99d61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b1002f672a472e9cb9ca956f6a6918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9d1cf17db764225a758045c81f89c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3093ad1cc6d54dbb9673ea1f05d7191c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c423438c54e4b04a996e40dd0c8c3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8955d148f43a4671836b322bff74716d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c0df71ebd2f421894862a3d666c1e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fbb7a1980084dfc93979bd003a3b906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee55ce6ec7a2426dbd3de27b056c526a",
              "IPY_MODEL_ef535c5cb87641a594d870a4b940206f",
              "IPY_MODEL_ffeedbc6d8d54721a8d18c9865477f45"
            ],
            "layout": "IPY_MODEL_73228821d5644299bf49665f02231550"
          }
        },
        "ee55ce6ec7a2426dbd3de27b056c526a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1bd232e3e484ad293ec961218a6826f",
            "placeholder": "​",
            "style": "IPY_MODEL_cad11af2157a484fbda448f6510dac75",
            "value": "Loading weights: 100%"
          }
        },
        "ef535c5cb87641a594d870a4b940206f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15d05328aca14df9b311ae0b91682250",
            "max": 105,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d89f29748c9d4148bef751eacc8e7605",
            "value": 105
          }
        },
        "ffeedbc6d8d54721a8d18c9865477f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df983884fa34f099e1dbc04eb850bcd",
            "placeholder": "​",
            "style": "IPY_MODEL_b31634dd395e4375a2bb633d5eec6691",
            "value": " 105/105 [00:00&lt;00:00, 350.96it/s, Materializing param=classifier.weight]"
          }
        },
        "73228821d5644299bf49665f02231550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1bd232e3e484ad293ec961218a6826f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad11af2157a484fbda448f6510dac75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15d05328aca14df9b311ae0b91682250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d89f29748c9d4148bef751eacc8e7605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7df983884fa34f099e1dbc04eb850bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b31634dd395e4375a2bb633d5eec6691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}